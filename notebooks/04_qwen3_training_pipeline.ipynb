{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3 Training Pipeline for Astrabot\n",
    "\n",
    "This notebook demonstrates how to fine-tune Qwen3 models on your personal conversation data using Unsloth and advanced training techniques.\n",
    "\n",
    "## Features\n",
    "- üöÄ Efficient 4-bit quantization with Unsloth\n",
    "- üéØ Multiple training data formats (conversational, adaptive, burst, Q&A)\n",
    "- üß† Reasoning capability with thinking tags\n",
    "- üë• Partner-specific style adaptation\n",
    "- üê¶ Twitter content enhancement\n",
    "- üìä Multi-stage training support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install unsloth\n",
    "!pip install transformers trl datasets peft accelerate\n",
    "!pip install pandas numpy pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import Astrabot modules\n",
    "from src.llm.training_data_creator import TrainingDataCreator\n",
    "from src.llm.adaptive_trainer import AdaptiveTrainer\n",
    "from src.core.style_analyzer import analyze_all_communication_styles\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "# Setup logging\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training configuration\n",
    "config_path = project_root / \"configs\" / \"training_config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Display key configuration parameters\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Base model: {config['model']['name']}\")\n",
    "print(f\"  Max sequence length: {config['model']['max_seq_length']}\")\n",
    "print(f\"  4-bit quantization: {config['model']['load_in_4bit']}\")\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank (r): {config['lora']['r']}\")\n",
    "print(f\"  Alpha: {config['lora']['alpha']}\")\n",
    "print(f\"  Target modules: {', '.join(config['lora']['target_modules'])}\")\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {config['training']['num_train_epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Signal data\n",
    "data_path = project_root / \"data\" / \"raw\" / \"signal-flatfiles\"\n",
    "messages_path = data_path / \"signal.csv\"\n",
    "recipients_path = data_path / \"recipient.csv\"\n",
    "\n",
    "# Check if files exist\n",
    "if not messages_path.exists() or not recipients_path.exists():\n",
    "    print(\"‚ö†Ô∏è  Signal data not found. Please run the extraction process first.\")\n",
    "    print(f\"Expected paths:\\n  {messages_path}\\n  {recipients_path}\")\n",
    "else:\n",
    "    messages_df = pd.read_csv(messages_path)\n",
    "    recipients_df = pd.read_csv(recipients_path)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(messages_df):,} messages\")\n",
    "    print(f\"‚úÖ Found {len(recipients_df):,} recipients\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nMessage Statistics:\")\n",
    "    print(f\"  Date range: {pd.to_datetime(messages_df['date_sent'], unit='ms').min()} to {pd.to_datetime(messages_df['date_sent'], unit='ms').max()}\")\n",
    "    print(f\"  Unique threads: {messages_df['thread_id'].nunique():,}\")\n",
    "    print(f\"  Messages with text: {messages_df['body'].notna().sum():,}\")\n",
    "    print(f\"  Average message length: {messages_df['body'].dropna().str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze communication styles\n",
    "YOUR_RECIPIENT_ID = 2  # Update this to your actual ID\n",
    "\n",
    "print(\"Analyzing communication styles...\")\n",
    "communication_styles = analyze_all_communication_styles(\n",
    "    messages_df, recipients_df, YOUR_RECIPIENT_ID\n",
    ")\n",
    "\n",
    "# Display style breakdown\n",
    "print(f\"\\nAnalyzed {len(communication_styles)} conversation partners:\")\n",
    "style_counts = {}\n",
    "for recipient_id, style_info in communication_styles.items():\n",
    "    style_type = style_info.get('style_type', 'unknown')\n",
    "    style_counts[style_type] = style_counts.get(style_type, 0) + 1\n",
    "\n",
    "for style, count in sorted(style_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {style}: {count} people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training data creator\n",
    "creator = TrainingDataCreator(YOUR_RECIPIENT_ID)\n",
    "adaptive_trainer = AdaptiveTrainer(YOUR_RECIPIENT_ID)\n",
    "\n",
    "# Create different types of training data\n",
    "all_training_examples = []\n",
    "\n",
    "# 1. Conversational data\n",
    "print(\"Creating conversational training data...\")\n",
    "conv_data = creator.create_conversational_training_data(\n",
    "    messages_df, \n",
    "    recipients_df,\n",
    "    context_window=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "print(f\"  Created {len(conv_data)} conversational examples\")\n",
    "\n",
    "# Show sample\n",
    "if conv_data:\n",
    "    sample = conv_data[0]\n",
    "    print(\"\\nSample conversational example:\")\n",
    "    print(f\"  Messages: {len(sample['messages'])} turns\")\n",
    "    if 'metadata' in sample:\n",
    "        print(f\"  Type: {sample['metadata'].get('type', 'unknown')}\")\n",
    "        print(f\"  Has Twitter: {sample['metadata'].get('has_twitter', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Adaptive training data\n",
    "print(\"Creating adaptive training data...\")\n",
    "adaptive_data = adaptive_trainer.create_adaptive_training_data(\n",
    "    messages_df, recipients_df, communication_styles\n",
    ")\n",
    "print(f\"  Created {len(adaptive_data)} adaptive examples\")\n",
    "\n",
    "# Show breakdown by partner style\n",
    "if adaptive_data:\n",
    "    style_breakdown = {}\n",
    "    for ex in adaptive_data[:100]:  # Sample first 100\n",
    "        style = ex.get('other_person_style', 'unknown')\n",
    "        style_breakdown[style] = style_breakdown.get(style, 0) + 1\n",
    "    \n",
    "    print(\"\\nAdaptive examples by style (first 100):\")\n",
    "    for style, count in sorted(style_breakdown.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {style}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Burst sequence data\n",
    "print(\"Creating burst sequence data...\")\n",
    "burst_data = creator.create_burst_sequence_data(\n",
    "    messages_df.to_dict('records')[:1000],  # Sample for speed\n",
    "    YOUR_RECIPIENT_ID\n",
    ")\n",
    "print(f\"  Created {len(burst_data)} burst sequence examples\")\n",
    "\n",
    "# 4. Q&A data\n",
    "print(\"\\nCreating Q&A training data...\")\n",
    "qa_data = creator.create_qa_training_data(\n",
    "    messages_df.to_dict('records')[:1000],  # Sample for speed\n",
    "    YOUR_RECIPIENT_ID\n",
    ")\n",
    "print(f\"  Created {len(qa_data)} Q&A examples\")\n",
    "\n",
    "# Combine all data with configured weights\n",
    "dataset_config = config['dataset']['modes']\n",
    "total_examples = 10000  # Target total examples\n",
    "\n",
    "final_examples = []\n",
    "if dataset_config['conversational']['enabled']:\n",
    "    num = int(total_examples * dataset_config['conversational']['weight'])\n",
    "    final_examples.extend(conv_data[:num])\n",
    "\n",
    "if dataset_config['adaptive']['enabled']:\n",
    "    num = int(total_examples * dataset_config['adaptive']['weight'])\n",
    "    final_examples.extend(adaptive_data[:num])\n",
    "\n",
    "if dataset_config['burst_sequence']['enabled']:\n",
    "    num = int(total_examples * dataset_config['burst_sequence']['weight'])\n",
    "    final_examples.extend(burst_data[:num])\n",
    "\n",
    "if dataset_config['qa']['enabled']:\n",
    "    num = int(total_examples * dataset_config['qa']['weight'])\n",
    "    final_examples.extend(qa_data[:num])\n",
    "\n",
    "print(f\"\\n‚úÖ Total training examples: {len(final_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Qwen3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model selection - adjust based on your GPU memory\n",
    "model_options = {\n",
    "    \"small\": \"unsloth/Qwen3-3B\",     # ~6GB VRAM\n",
    "    \"medium\": \"unsloth/Qwen3-8B\",    # ~16GB VRAM\n",
    "    \"large\": \"unsloth/Qwen3-14B\"     # ~28GB VRAM\n",
    "}\n",
    "\n",
    "# Select model size\n",
    "MODEL_SIZE = \"small\"  # Change this based on your hardware\n",
    "MODEL_NAME = model_options[MODEL_SIZE]\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=config['model']['max_seq_length'],\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=config['model']['load_in_4bit'],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=config['lora']['r'],\n",
    "    target_modules=config['lora']['target_modules'],\n",
    "    lora_alpha=config['lora']['alpha'],\n",
    "    lora_dropout=config['lora']['dropout'],\n",
    "    bias=config['lora']['bias'],\n",
    "    use_gradient_checkpointing=config['lora']['use_gradient_checkpointing'],\n",
    "    random_state=config['lora']['random_state'],\n",
    "    use_rslora=config['lora']['use_rslora'],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters applied\")\n",
    "\n",
    "# Show model info\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Qwen3 chat template to training examples\n",
    "print(\"Applying chat template to training data...\")\n",
    "\n",
    "formatted_examples = []\n",
    "skipped = 0\n",
    "\n",
    "for i, example in enumerate(final_examples):\n",
    "    try:\n",
    "        if 'messages' in example:\n",
    "            # Already in chat format\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                example['messages'],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        elif 'instruction' in example and 'output' in example:\n",
    "            # Convert to chat format\n",
    "            messages = []\n",
    "            \n",
    "            # Add system message for complex instructions\n",
    "            if 'adapt' in example.get('instruction', '').lower():\n",
    "                messages.append({\n",
    "                    'role': 'system',\n",
    "                    'content': example['instruction']\n",
    "                })\n",
    "                if 'input' in example:\n",
    "                    messages.append({\n",
    "                        'role': 'user',\n",
    "                        'content': example['input']\n",
    "                    })\n",
    "            else:\n",
    "                messages.append({\n",
    "                    'role': 'user',\n",
    "                    'content': example.get('input', example['instruction'])\n",
    "                })\n",
    "            \n",
    "            messages.append({\n",
    "                'role': 'assistant',\n",
    "                'content': example['output']\n",
    "            })\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        else:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        formatted_examples.append({\n",
    "            'text': text,\n",
    "            'metadata': example.get('metadata', {})\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {e}\")\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_examples)} examples\")\n",
    "if skipped > 0:\n",
    "    print(f\"‚ö†Ô∏è  Skipped {skipped} invalid examples\")\n",
    "\n",
    "# Show sample formatted text\n",
    "if formatted_examples:\n",
    "    print(\"\\nSample formatted text (first 500 chars):\")\n",
    "    print(formatted_examples[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Add optional reasoning examples\n",
    "if config['reasoning']['enabled']:\n",
    "    print(\"Adding reasoning examples...\")\n",
    "    \n",
    "    # Create some reasoning examples\n",
    "    reasoning_examples = [\n",
    "        {\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': 'Explain step by step: If a train travels 120 miles in 2 hours, what is its average speed?'},\n",
    "                {'role': 'assistant', 'content': '<think>\\nTo find average speed, I need to divide distance by time.\\nDistance = 120 miles\\nTime = 2 hours\\nSpeed = Distance / Time = 120 / 2 = 60\\n</think>\\n\\nTo find the average speed, I\\'ll divide the total distance by the total time:\\n\\nDistance = 120 miles\\nTime = 2 hours\\n\\nAverage speed = Distance √∑ Time\\nAverage speed = 120 miles √∑ 2 hours\\nAverage speed = 60 miles per hour\\n\\nTherefore, the train\\'s average speed is 60 mph.'}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': 'Why do leaves change color in autumn?'},\n",
    "                {'role': 'assistant', 'content': '<think>\\nLeaves change color because:\\n1. Chlorophyll breaks down\\n2. Other pigments become visible\\n3. Temperature and daylight changes trigger this\\n4. Trees prepare for winter\\n</think>\\n\\nLeaves change color in autumn due to chemical changes as trees prepare for winter. During growing season, chlorophyll (which makes leaves green) dominates. As days shorten and temperatures drop, chlorophyll breaks down and stops being produced. This reveals other pigments that were always present: carotenoids (yellows and oranges) and anthocyanins (reds and purples). The specific colors depend on the tree species, weather conditions, and soil pH.'}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format reasoning examples\n",
    "    for ex in reasoning_examples:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            ex['messages'],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        formatted_examples.append({'text': text, 'metadata': {'type': 'reasoning'}})\n",
    "    \n",
    "    print(f\"  Added {len(reasoning_examples)} reasoning examples\")\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_list(formatted_examples)\n",
    "\n",
    "print(f\"\\n‚úÖ Created dataset with {len(dataset)} examples\")\n",
    "print(f\"Dataset features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3-output\",\n",
    "    num_train_epochs=1,  # Start with 1 epoch for testing\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",\n",
    "    max_steps=50,  # Limit steps for notebook demo\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=config['model']['max_seq_length'],\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current memory stats\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU: {gpu_stats.name}\")\n",
    "    print(f\"Max memory: {max_memory} GB\")\n",
    "    print(f\"Reserved memory: {start_gpu_memory} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, training will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show training stats\n",
    "print(f\"\\nTraining completed in {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    print(f\"\\nPeak memory usage: {used_memory} GB\")\n",
    "    print(f\"Memory for training: {used_memory_for_lora} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable fast inference\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Hey! How's your day going?\",\n",
    "    \"Can you explain what machine learning is?\",\n",
    "    \"What do you think about the weather today?\",\n",
    "    \"Solve this: If I have 15 apples and give away 7, how many do I have left?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Determine if reasoning is needed\n",
    "    needs_reasoning = any(word in prompt.lower() for word in ['explain', 'solve', 'why', 'how'])\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        enable_thinking=needs_reasoning  # Enable thinking for reasoning tasks\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate with appropriate settings\n",
    "    if needs_reasoning:\n",
    "        temp = config['reasoning']['temperature']\n",
    "        top_p = config['reasoning']['top_p']\n",
    "    else:\n",
    "        temp = config['chat']['temperature']\n",
    "        top_p = config['chat']['top_p']\n",
    "    \n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    _ = model.generate(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "output_dir = \"./qwen3-finetuned\"\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'base_model': MODEL_NAME,\n",
    "    'training_examples': len(dataset),\n",
    "    'training_steps': trainer_stats.metrics.get('train_steps', 50),\n",
    "    'final_loss': trainer_stats.metrics['train_loss'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'training_info.json'), 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining info saved to training_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Save merged 16-bit model (larger, more compatible)\n",
    "save_16bit = False  # Set to True if you want this\n",
    "\n",
    "if save_16bit:\n",
    "    print(\"Saving merged 16-bit model...\")\n",
    "    model.save_pretrained_merged(\n",
    "        \"qwen3-merged-16bit\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\"\n",
    "    )\n",
    "    print(\"‚úÖ Saved merged 16-bit model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Export to GGUF for llama.cpp / Ollama\n",
    "export_gguf = False  # Set to True if you want this\n",
    "\n",
    "if export_gguf:\n",
    "    print(\"Exporting to GGUF format...\")\n",
    "    \n",
    "    # Choose quantization method\n",
    "    quant_method = \"q4_k_m\"  # Recommended balance of size/quality\n",
    "    \n",
    "    model.save_pretrained_gguf(\n",
    "        f\"qwen3-{quant_method}\",\n",
    "        tokenizer,\n",
    "        quantization_method=quant_method\n",
    "    )\n",
    "    print(f\"‚úÖ Exported GGUF model with {quant_method} quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "### Running Full Training\n",
    "For production training with all your data:\n",
    "\n",
    "```bash\n",
    "python scripts/train_qwen3.py \\\n",
    "  --config configs/training_config.yaml \\\n",
    "  --messages data/raw/signal-flatfiles/signal.csv \\\n",
    "  --recipients data/raw/signal-flatfiles/recipient.csv \\\n",
    "  --output ./models/qwen3-personal \\\n",
    "  --test\n",
    "```\n",
    "\n",
    "### Multi-Stage Training\n",
    "For advanced multi-stage training:\n",
    "\n",
    "```bash\n",
    "python scripts/train_qwen3.py \\\n",
    "  --config configs/training_config.yaml \\\n",
    "  --multi-stage \\\n",
    "  --output ./models/qwen3-multistage\n",
    "```\n",
    "\n",
    "### Tips for Better Results\n",
    "\n",
    "1. **Data Quality**: \n",
    "   - Remove very short messages\n",
    "   - Filter out messages with only links/media\n",
    "   - Include diverse conversation types\n",
    "\n",
    "2. **Training Duration**:\n",
    "   - Start with 1 epoch and evaluate\n",
    "   - Watch for overfitting (loss stops decreasing)\n",
    "   - Use validation set if available\n",
    "\n",
    "3. **Model Size**:\n",
    "   - 3B: Good for most personal use, fast inference\n",
    "   - 8B: Better quality, needs ~16GB VRAM\n",
    "   - 14B: Best quality, needs ~28GB VRAM\n",
    "\n",
    "4. **Inference Settings**:\n",
    "   - Reasoning: temp=0.6, top_p=0.95\n",
    "   - Chat: temp=0.7, top_p=0.8\n",
    "   - Adjust based on your preference\n",
    "\n",
    "5. **Privacy**:\n",
    "   - Models contain your conversation patterns\n",
    "   - Don't share publicly without careful consideration\n",
    "   - Consider training separate models for different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Pipeline Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training examples: {len(dataset)}\")\n",
    "print(f\"LoRA rank: {config['lora']['r']}\")\n",
    "print(f\"Training steps: {trainer_stats.metrics.get('train_steps', 'N/A')}\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(\"\\n‚úÖ Training complete! Your personalized Qwen3 model is ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
